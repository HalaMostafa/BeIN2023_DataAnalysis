{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types and Missing Values\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Objectives\n",
    "\n",
    "+ Know all the possible column data types\n",
    "+ Know that each value in a column must be of the same data type\n",
    "+ Know the representations of missing values and which ones are used for each data type\n",
    "+ Know how to get metadata with `shape`, `size`, and `info`\n",
    "\n",
    "## Data  Types \n",
    "Each column of data in a pandas DataFrame has a data type. This is a very similar concept to types in Python. Just like every object has a type, every column has a data type. \n",
    "\n",
    "### Most Common Data Types\n",
    "The following are the most common data types that appear frequently in DataFrames. \n",
    "\n",
    "* boolean\n",
    "* integer\n",
    "* float\n",
    "* object (mainly strings)\n",
    "* datetime (a specific moment in time)\n",
    "\n",
    "### Other Data Types\n",
    "There are three other data types that are less common. We will cover them when necessary.\n",
    "\n",
    "* category\n",
    "* timedelta (a specific amount of time)\n",
    "* period (a specific time period)\n",
    "\n",
    "### More on the most common data types\n",
    "\n",
    "#### boolean\n",
    "Boolean columns contain only two values: `True` and `False`\n",
    "\n",
    "#### integer\n",
    "Whole numbers without a decimal\n",
    "\n",
    "#### float\n",
    "Numbers with decimals\n",
    "\n",
    "#### object\n",
    "The object data type is a bit confusing. Each value in an object column can be **any** Python object. But, nearly all of the time, object data type columns contain strings. They can contain any other Python object such as integers, floats, or even complex types such as lists or dictionaries. There is no specific data type for strings as there are in most other data processing packages. When you see that object is the data type, you should assume you have a string column.\n",
    "\n",
    "#### datetime\n",
    "A datetime is a specific moment in time with both a date (month, year, day) and a time (hour, minute, second, fraction of a second). All datetimes in pandas have nanosecond (1 billionth of a second) precision.\n",
    "\n",
    "\n",
    "### The importance of knowing the data type\n",
    "\n",
    "Knowing the data type of each column of your pandas DataFrame is very important. The main reason for this is that every value in each column will be of the same type. For instance, if you select a single value from a column that has an integer data type, then you are guaranteed that this value is also an integer.  Knowing the data type of a column is one of the most fundamental pieces of knowledge of your DataFrame.\n",
    "\n",
    "### A major exception with the object data type\n",
    "\n",
    "The object data type, is unfortunately, an exception to the rule in the previous section. Although columns that have object data type are typically strings, there is no guarantee that each value will be a string. You could very well have an integer, list, or even another DataFrame as a value in an object column.\n",
    "\n",
    "## Missing Values\n",
    "\n",
    "Missing value representation is actually a fairly complex issue. If you are curious you can read a [small manifesto][1] on it from the numpy developers.\n",
    "\n",
    "### Missing Value Representation -  `NaN`,  `None`, and `NaT`\n",
    "pandas represents missing values differently based on the data type of the column.\n",
    "\n",
    "* `NaN` stands for not a number and is technically a floating point value\n",
    "* `None` is the literal Python object `None`. This will only be found in object columns\n",
    "* `NaT` stands for not a time and is used for missing values in datetime, timedelta, and period columns\n",
    "\n",
    "### Missing values for each data type\n",
    "\n",
    "* **boolean and integer** - No representation for missing values exist for boolean and integer columns. This is unfortunate, but a current limitation (See the next section for an update on this limitation). If you have boolean or integer columns and need to also have missing values within them, then these columns will be coerced to a float.\n",
    "\n",
    "* **floats** - Use only `NaN` as the missing value.  \n",
    "\n",
    "* **object** - Columns of object data type can contain any Python object so technically you may see `NaN`, `None`, or `NaT` but primarily you will see `None`.\n",
    "\n",
    "* **datetime, timedelta, period** - Use only `NaT` as the missing value.\n",
    "\n",
    "### Integer NaN update for pandas 0.24\n",
    "\n",
    "With the release of pandas version 0.24 (February 2019), missing value representation was made possible for a special kind of integer data type called **Int64Dtype**. There is still no missing value representation for the default integer data type. For more on this new addition, see chapter **X**.\n",
    "\n",
    "### Even more data types\n",
    "\n",
    "pandas is a library under constant flux, and has implemented a couple newer data types for very specific situations. The **SparseDType** is used for data that contains a vast majority of values as 0 or some other constant. The other is **IntervalDtype** for creating values that represent an entire interval, such as all the values between 3 and 5. Both of these data types are newer additions to the library, are rarely used, and will be covered in chapter **X**.\n",
    "\n",
    "## Finding the data types of each column\n",
    "The `dtypes` DataFrame attribute (NOT a method) returns the data type of each column. Let's get the data types of our `bikes` DataFrame. Note that the returned data is a Series with the column names now in the index and the data type as the values.\n",
    "\n",
    "[1]: https://docs.scipy.org/doc/numpy/neps/missing-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bikes = pd.read_csv('data/bikes.csv')\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think string whenever you see object\n",
    "pandas does not have a string data type like most databases, but when you see **object** you should assume that the column consists entirely of strings.\n",
    "\n",
    "### Why are `starttime` and `stoptime` object data types?\n",
    "If you look at the output of the `bikes` DataFrame, it's apparent that both the `starttime` and `stoptime` columns are datetimes but the output from `dtypes` from above states that they are objects.\n",
    "\n",
    "When reading in a text file, such as `bikes.csv`, you must explicitly tell pandas which columns are datetimes. We can force pandas to read these columns as datetimes with the `parse_dates` parameter of the `read_csv` function. It accepts a list of the column names we would like to make datetimes. Let's reread the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are all those 64's at the end of the data types?\n",
    "Booleans, integers, floats, datetimes and timedeltas all use a particular amount of memory for each of their values. The memory is measured in **bits**. By default, pandas uses 64 bits to represent integers, floats, datetimes, and timedeltas. pandas will use 64 bits regardless if you have a 32 or 64 bit machine.\n",
    "\n",
    "It is possible to use a different number of bits for integers and floats.  Integers can be either 8, 16, 32, or 64 bits while floats can be 16, 32, 64, or 128. For instance, a 128-bit float column will show up as `float128`. \n",
    "\n",
    "Technically a `float128` is a different data type than a `float64` but generally you will not have to worry about such a distinction as the operations between different float columns will be the same. It's also rare to see anything other than 64 bit integers or floats since that is the default. You would need to manually change their size to get a different type.\n",
    "\n",
    "**Booleans** are stored as 8-bits, which is a single **byte**. Datetimes and timedeltas are always stored as 64 bits. **Objects** can store any Python object, so there is no set amount of memory for each of their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more metadata\n",
    "Metadata can be defined as data on the data. The data type of each column is an example of **metadata**. The number of rows and columns is another piece of metadata. We find this with the `shape` attribute, which returns a tuple of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of elements with the `size` attribute\n",
    "The `size` attribute returns the total number of elements (the number of columns multiplied by the number of rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Types plus more with the `info` method\n",
    "The `info` DataFrame method returns output similar to `dtypes`, but also returns the number of non-missing values in each column along with more info such as the \n",
    "* Type of object (always a DataFrame)\n",
    "* The type of index and number of rows\n",
    "* The number of columns\n",
    "* The data types of each column and the number of non-missing (a.k.a non-null)\n",
    "* The frequency count of all data types\n",
    "* The total memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ways to check missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.isna()\n",
    "#precentage of missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating methods\n",
    "\n",
    "- The technical definition of an **aggregation** is when `a sequence of values is summarized by a single number`. \n",
    "\n",
    "- For example, `sum`, `mean`, `median`, `min`, and `max` are all examples of aggregation functions. \n",
    "\n",
    "- By default, calling these methods on a pandas DataFrame will apply the aggregation to each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Below`, we use a dataset containing the percentage of undergraduate races for all US colleges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv('data/college.csv', index_col='instnm')\n",
    "college.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = college.loc[:, 'ugds_white':'ugds_unkn']\n",
    "cr.head()\n",
    "#mean()/min()/max()/round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating within groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = pd.read_csv('data/insurance.csv')\n",
    "ins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency of unique values in a single column**\n",
    "\n",
    "One of the simplest aggregations is the frequency of occurrence of all the unique values within a single column. This\n",
    "is performed below with the value_counts method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**single group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multiple group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>ATL</th>\n",
       "      <th>DEN</th>\n",
       "      <th>DFW</th>\n",
       "      <th>IAH</th>\n",
       "      <th>LAS</th>\n",
       "      <th>LAX</th>\n",
       "      <th>MSP</th>\n",
       "      <th>ORD</th>\n",
       "      <th>PHX</th>\n",
       "      <th>SFO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AS</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline  ATL   DEN  DFW   IAH   LAS  LAX  MSP   ORD   PHX  SFO\n",
       "0      AA  4.0   9.0  5.0  11.0   8.0  3.0  1.0   8.0   5.0  3.0\n",
       "1      AS  6.0  -3.0 -5.0   1.0   2.0 -3.0  6.0   2.0  -9.0  4.0\n",
       "2      B6  NaN  12.0  4.0   NaN  11.0  2.0  NaN  23.0  20.0  5.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = pd.read_csv('data/tidy/average_arrival_delay.csv')\n",
    "pc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
